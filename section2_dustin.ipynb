{
 "cells": [
  {
   "source": [
    "# Section 2: Analyzing Causal Forests on Simulated Data similar to GSS data "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from econml.dml import CausalForestDML\n",
    "\n",
    "from copy import deepcopy\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def fullDisplay():\n",
    "    pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "\n",
    "def defaultDisplay():\n",
    "    pd.reset_option('^display.', silent=True)"
   ]
  },
  {
   "source": [
    "## Loading data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       year    id  wrkstat      hrs1      hrs2  evwork  occ  prestige  wrkslf  \\\n",
       "0         0     1        7  0.004845  0.005228       1  135  0.005641       2   \n",
       "1         0     2        1  0.005055  0.005228       0  106  0.006538       2   \n",
       "2         0     3        7  0.004845  0.005228       1   99  0.006538       2   \n",
       "3         0     4        3  0.005055  0.005228       0  142  0.004615       2   \n",
       "4         0     5        8  0.005055  0.005228       1  211  0.005171       2   \n",
       "...     ...   ...      ...       ...       ...     ...  ...       ...     ...   \n",
       "36496    15  2040        3  0.005055  0.005228       0  211  0.005171       2   \n",
       "36497    15  2041        3  0.005055  0.005228       0  211  0.005171       2   \n",
       "36498    15  2042        7  0.004845  0.005228       1  211  0.005171       2   \n",
       "36499    15  2043        7  0.005935  0.005228       1  211  0.005171       2   \n",
       "36500    15  2044        7  0.004845  0.005228       1  211  0.005171       2   \n",
       "\n",
       "       wrkgovt  ...  adults_miss  unrelat_miss  earnrs_miss  income_miss  \\\n",
       "0            2  ...            0             0            0            0   \n",
       "1            2  ...            0             1            0            0   \n",
       "2            2  ...            0             1            0            0   \n",
       "3            0  ...            0             0            0            0   \n",
       "4            1  ...            0             0            0            0   \n",
       "...        ...  ...          ...           ...          ...          ...   \n",
       "36496        2  ...            0             0            0            0   \n",
       "36497        2  ...            0             0            0            0   \n",
       "36498        2  ...            0             1            0            0   \n",
       "36499        2  ...            0             1            0            0   \n",
       "36500        2  ...            0             1            0            0   \n",
       "\n",
       "       rincome_miss  income86_miss  partyid_miss  polviews_miss  attblack  \\\n",
       "0                 0              0             0              0  0.005440   \n",
       "1                 1              0             0              0  0.004080   \n",
       "2                 0              0             0              0  0.002040   \n",
       "3                 1              0             0              0  0.004080   \n",
       "4                 0              0             0              0  0.004080   \n",
       "...             ...            ...           ...            ...       ...   \n",
       "36496             1              1             0              0  0.004080   \n",
       "36497             1              1             0              0  0.006120   \n",
       "36498             0              1             0              0  0.004080   \n",
       "36499             1              1             0              0  0.005021   \n",
       "36500             0              1             0              0  0.008160   \n",
       "\n",
       "       attblack_miss  \n",
       "0                  0  \n",
       "1                  0  \n",
       "2                  0  \n",
       "3                  0  \n",
       "4                  0  \n",
       "...              ...  \n",
       "36496              0  \n",
       "36497              0  \n",
       "36498              0  \n",
       "36499              1  \n",
       "36500              0  \n",
       "\n",
       "[36501 rows x 120 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>year</th>\n      <th>id</th>\n      <th>wrkstat</th>\n      <th>hrs1</th>\n      <th>hrs2</th>\n      <th>evwork</th>\n      <th>occ</th>\n      <th>prestige</th>\n      <th>wrkslf</th>\n      <th>wrkgovt</th>\n      <th>...</th>\n      <th>adults_miss</th>\n      <th>unrelat_miss</th>\n      <th>earnrs_miss</th>\n      <th>income_miss</th>\n      <th>rincome_miss</th>\n      <th>income86_miss</th>\n      <th>partyid_miss</th>\n      <th>polviews_miss</th>\n      <th>attblack</th>\n      <th>attblack_miss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n      <td>7</td>\n      <td>0.004845</td>\n      <td>0.005228</td>\n      <td>1</td>\n      <td>135</td>\n      <td>0.005641</td>\n      <td>2</td>\n      <td>2</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.005440</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0.005055</td>\n      <td>0.005228</td>\n      <td>0</td>\n      <td>106</td>\n      <td>0.006538</td>\n      <td>2</td>\n      <td>2</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.004080</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>3</td>\n      <td>7</td>\n      <td>0.004845</td>\n      <td>0.005228</td>\n      <td>1</td>\n      <td>99</td>\n      <td>0.006538</td>\n      <td>2</td>\n      <td>2</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.002040</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>4</td>\n      <td>3</td>\n      <td>0.005055</td>\n      <td>0.005228</td>\n      <td>0</td>\n      <td>142</td>\n      <td>0.004615</td>\n      <td>2</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.004080</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>5</td>\n      <td>8</td>\n      <td>0.005055</td>\n      <td>0.005228</td>\n      <td>1</td>\n      <td>211</td>\n      <td>0.005171</td>\n      <td>2</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.004080</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>36496</th>\n      <td>15</td>\n      <td>2040</td>\n      <td>3</td>\n      <td>0.005055</td>\n      <td>0.005228</td>\n      <td>0</td>\n      <td>211</td>\n      <td>0.005171</td>\n      <td>2</td>\n      <td>2</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.004080</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>36497</th>\n      <td>15</td>\n      <td>2041</td>\n      <td>3</td>\n      <td>0.005055</td>\n      <td>0.005228</td>\n      <td>0</td>\n      <td>211</td>\n      <td>0.005171</td>\n      <td>2</td>\n      <td>2</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.006120</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>36498</th>\n      <td>15</td>\n      <td>2042</td>\n      <td>7</td>\n      <td>0.004845</td>\n      <td>0.005228</td>\n      <td>1</td>\n      <td>211</td>\n      <td>0.005171</td>\n      <td>2</td>\n      <td>2</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.004080</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>36499</th>\n      <td>15</td>\n      <td>2043</td>\n      <td>7</td>\n      <td>0.005935</td>\n      <td>0.005228</td>\n      <td>1</td>\n      <td>211</td>\n      <td>0.005171</td>\n      <td>2</td>\n      <td>2</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.005021</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>36500</th>\n      <td>15</td>\n      <td>2044</td>\n      <td>7</td>\n      <td>0.004845</td>\n      <td>0.005228</td>\n      <td>1</td>\n      <td>211</td>\n      <td>0.005171</td>\n      <td>2</td>\n      <td>2</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.008160</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>36501 rows × 120 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "welfare = pd.read_csv(\"Data/welfare_clean.csv\", low_memory=False)\n",
    "treatments = welfare['w']\n",
    "labels = welfare['y']\n",
    "welfare.drop(columns=['w', 'y'], inplace=True)\n",
    "welfare"
   ]
  },
  {
   "source": [
    "## DGP, Estimation, and Evaluation functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dgp(welfare, effect_type=\"heterogeneous\", effect_homogeneous=10, effect_heterogeneous=2,\n",
    "        treatment_type=\"binary\", treatment_probability=0.5, heterogeneous_select=4, overlap=True,\n",
    "        overlap_percent=0.5, order=3, linearity=\"med\", N=30000):    \n",
    "    \n",
    "    featureNames = list(welfare.columns)\n",
    "\n",
    "    # Define non-confounders\n",
    "    importantFeatureNames = ['wrkstat', 'race', 'year', 'hrs1', 'income', 'occ80', 'id', 'educ'] # top 8 important features based on Shapley visualization from \n",
    "    importantFeatureIndices = []\n",
    "    for name in importantFeatureNames:\n",
    "        importantFeatureIndices.append(featureNames.index(name)) \n",
    "\n",
    "    # Error terms\n",
    "    error = np.random.normal(size=(N,1))\n",
    "\n",
    "    # Data generation\n",
    "    cov = welfare.cov()\n",
    "    means = welfare.mean(axis=0)\n",
    "    X = np.random.multivariate_normal(means.values, cov, size=N, check_valid='warn', tol=1e-8)\n",
    "\n",
    "    # Linearity specification\n",
    "    if linearity != 'full':\n",
    "        select = 0 # select n most important features for interactions and polynomials\n",
    "        poly = PolynomialFeatures(degree=order, interaction_only=False, include_bias=False)\n",
    "\n",
    "        if linearity == \"high\": \n",
    "            select = 2\n",
    "        elif linearity == \"med\": \n",
    "            select = 4\n",
    "        elif linearity == \"low\": \n",
    "            select = 8\n",
    "        else: # if some typo, assume baseline of high\n",
    "            select = 2\n",
    "\n",
    "        poly.fit(X[:, importantFeatureIndices[:select]])\n",
    "        fullData = poly.transform(X[:, importantFeatureIndices[:select]])\n",
    "        fullNames = poly.get_feature_names(input_features=importantFeatureNames[:select])\n",
    "        higherData = fullData[:, -(fullData.shape[1] - select):] # select only higher order\n",
    "        higherNames = fullNames[-(len(fullNames) - select):]\n",
    "\n",
    "        X = np.append(X, higherData, axis=1) \n",
    "        featureNames.extend(list(higherNames))\n",
    "\n",
    "    # Treatment selection\n",
    "    if treatment_type == \"binary\":\n",
    "        # randomly assigned treatments with propensity treatment_probability\n",
    "        treatments = np.random.choice([0, 1], size=N, p=[1 - treatment_probability, treatment_probability]).reshape((-1, 1))\n",
    "        if not overlap:\n",
    "            forced = random.sample(list(np.arange(treatments.shape[0])), int(treatments.shape[0] * overlap_percent))\n",
    "            treatments[forced] = 0\n",
    "        treated = treatments > 0\n",
    "\n",
    "        # generate counterfactual treatment indicator vector\n",
    "        c_treatments = deepcopy(treatments)\n",
    "        c_treatments[treatments == 0] = 1\n",
    "        c_treatments[treatments == 1] = 0\n",
    "    else:\n",
    "        treatments = np.random.uniform(size=(N, 1))\n",
    "        if not overlap:\n",
    "            forced = random.sample(list(np.arange(treatments.shape[0])), int(treatments.shape[0] * overlap_percent))\n",
    "            treatments[forced] = 0\n",
    "        treated = treatments > 0.5\n",
    "\n",
    "        # to-do: counterfactual treatment vector\n",
    "        c_treatments = deepcopy(treatments)\n",
    "        c_treatments[treatments > 0] = 0\n",
    "        c_treatments[treatments == 0] = treatments.mean()\n",
    "\n",
    "    # Treatment effect calculation\n",
    "    if effect_type == \"homogeneous\":\n",
    "\n",
    "        T = treatments*effect_homogeneous\n",
    "        CT = c_treatments*effect_homogeneous\n",
    "\n",
    "    else: \n",
    "        # heterogeneous treatment is effect 1 + effect * (sum of first heterogeneous_select important variables)\n",
    "        heterogeneousIndices = importantFeatureIndices[:heterogeneous_select]\n",
    "\n",
    "        T = 1 + (effect_heterogeneous*(X[:, heterogeneousIndices].sum(axis=1)))*treatments.ravel()\n",
    "        T = T.reshape(-1, 1)\n",
    "        T[~treated] = np.zeros((~treated).sum())\n",
    "        \n",
    "        # counterfactual effect\n",
    "        CT = 1 + (effect_heterogeneous*(X[:, heterogeneousIndices].sum(axis=1)))*c_treatments.ravel()\n",
    "        CT = CT.reshape(-1, 1)\n",
    "        CT[treated] = np.zeros((treated).sum()) # everyone that was treated goes to untreated in counterfactual\n",
    "\n",
    "    # Outcome calculation\n",
    "    betas = np.random.normal(size=X.shape[1]).reshape(-1,1)\n",
    "    y = T + X@betas + error\n",
    "    cy = CT + X@betas + error\n",
    "    \n",
    "    # ATE calculation\n",
    "    empirical_treated = y[treated] - cy[treated] # the treated (in y)\n",
    "    empirical_untreated = cy[~treated] - y[~treated] # the untreated\n",
    "    ate = (empirical_treated.mean() + empirical_untreated.mean())/2\n",
    "    \n",
    "    return y, X, betas, featureNames, treatments, cy, ate\n",
    "\n",
    "def estimate_cf(y, X, treatments, test_size=0.2, criterion='mse', cv=5):\n",
    "    # split data into train and test sets \n",
    "    X_train, X_test, Y_train, Y_test, T_train, T_test = train_test_split(X, y, treatments, test_size=test_size)\n",
    "        \n",
    "    # specify hyperparams of model\n",
    "    est = CausalForestDML(criterion='het', \n",
    "                            n_estimators=1000,       \n",
    "                            max_samples=0.5,\n",
    "                            discrete_treatment=True,\n",
    "                            honest=True,\n",
    "                            inference=True,\n",
    "                            cv=cv,\n",
    "                            )\n",
    "    # fit model\n",
    "    est.fit(Y_train, T_train, X=X_train, W=None)\n",
    "        \n",
    "    return est, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute evaluation stats over K simulations\n",
    "def evalStats(estimators, trueATES, evalSets):\n",
    "    K = len(estimators)\n",
    "    \n",
    "    totalBias = 0\n",
    "    totalRMSE = 0\n",
    "    totalCoverage = 0\n",
    "    totalInterval = 0\n",
    "\n",
    "    for i in range(K):\n",
    "        estimateATE = estimators[i].ate(evalSets[i])\n",
    "        lb, ub = estimators[i].ate_interval(evalSets[i])\n",
    "        \n",
    "        bias = trueATES[i] - estimateATE\n",
    "        totalBias += bias\n",
    "        \n",
    "        rmse = bias**2\n",
    "        totalRMSE += rmse\n",
    "\n",
    "        coverage = 1 if (trueATES[i] >= lb and trueATES[i] <= ub) else 0\n",
    "        totalCoverage += coverage\n",
    "\n",
    "        interval = ub - lb\n",
    "        totalInterval += interval\n",
    "    \n",
    "    return totalBias / K, (totalRMSE / K)**0.5, totalCoverage / K, totalInterval / K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, X, trueBetas, featureNames, treatments, cy, ate = dgp(welfare, N=10000, order=3, treatment_probability=0.5)\n",
    "est, X_test = estimate_cf(y, X, treatments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 100"
   ]
  },
  {
   "source": [
    "## (1) Sample Size"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ns = [1000, 10000, 30000, 50000, 100000]\n",
    "\n",
    "biasNs = []\n",
    "rmseNs = []\n",
    "coverageNs = []\n",
    "intervalNs = []\n",
    "\n",
    "for N in Ns:\n",
    "    estNs = []\n",
    "    trueATENs = []\n",
    "    evalSetNs = []\n",
    "    for i in range(K):\n",
    "        y, X, trueBetas, featureNames, treatments, cy, ate = dgp(welfare, N=N)\n",
    "        est, X_test = estimate_cf(y, X, treatments)\n",
    "\n",
    "        estNs.append(est)\n",
    "        trueATENs.append(est.ate(X_test)[0])\n",
    "        evalSetNs.append(X_test)\n",
    "\n",
    "    bias, rmse, coverage, interval = evalStats(estNs, trueATENs, evalSetNs)\n",
    "    \n",
    "    biasNs.append(bias)\n",
    "    rmseNs.append(rmse)\n",
    "    coverageNs.append(coverage)\n",
    "    intervalNs.append(interval)"
   ]
  },
  {
   "source": [
    "## (2) Degree of non-linearity in X"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linearityLevels = ['full', 'high', 'med', 'low']\n",
    "\n",
    "biasLLs = []\n",
    "rmseLLs = []\n",
    "coverageLLs = []\n",
    "intervalLLs = []\n",
    "\n",
    "for level in linearityLevels:\n",
    "    estLLs = []\n",
    "    trueATELLs = []\n",
    "    evalSetLLs = []\n",
    "    for i in range(K):\n",
    "        y, X, trueBetas, featureNames, treatments, cy, ate = dgp(welfare, linearity=level)\n",
    "        est, X_test = estimate_cf(y, X, treatments)\n",
    "\n",
    "        estLLs.append(est)\n",
    "        trueATELLs.append(est.ate(X_test)[0])\n",
    "        evalSetLLs.append(X_test)\n",
    "\n",
    "    bias, rmse, coverage, interval = evalStats(estLLs, trueATELLs, evalSetLLs)\n",
    "    \n",
    "    biasLLs.append(bias)\n",
    "    rmseLLs.append(rmse)\n",
    "    coverageLLs.append(coverage)\n",
    "    intervalLLs.append(interval)"
   ]
  },
  {
   "source": [
    "## (3) Percentage Treated"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentTreats = [0.1, 0.5, 0.9, 1]\n",
    "\n",
    "biasPTs = []\n",
    "rmsePTs = []\n",
    "coveragePTs = []\n",
    "intervalPTs = []\n",
    "\n",
    "for percent in percentTreats:\n",
    "    estPTs = []\n",
    "    trueATEPTs = []\n",
    "    evalSetPTs = []\n",
    "    for i in range(K):\n",
    "        y, X, trueBetas, featureNames, treatments, cy, ate = dgp(welfare, treatment_probability=percent)\n",
    "        est, X_test = estimate_cf(y, X, treatments)\n",
    "\n",
    "        estPTs.append(est)\n",
    "        trueATEPTs.append(est.ate(X_test)[0])\n",
    "        evalSetPTs.append(X_test)\n",
    "\n",
    "    bias, rmse, coverage, interval = evalStats(estPTs, trueATEPTs, evalSetPTs)\n",
    "    \n",
    "    biasPTs.append(bias)\n",
    "    rmsePTs.append(rmse)\n",
    "    coveragePTs.append(coverage)\n",
    "    intervalPTs.append(interval)"
   ]
  },
  {
   "source": [
    "## (4) Overlap"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlapTypes = [True, False]\n",
    "\n",
    "biasOTs = []\n",
    "rmseOTs = []\n",
    "coverageOTs = []\n",
    "intervalOTs = []\n",
    "\n",
    "for overlap in overlapTypes:\n",
    "    estOTs = []\n",
    "    trueATEOTs = []\n",
    "    evalSetOTs = []\n",
    "    for i in range(K):\n",
    "        y, X, trueBetas, featureNames, treatments, cy, ate = dgp(welfare, overlap=overlap)\n",
    "        est, X_test = estimate_cf(y, X, treatments)\n",
    "\n",
    "        estOTs.append(est)\n",
    "        trueATEOTs.append(est.ate(X_test)[0])\n",
    "        evalSetOTs.append(X_test)\n",
    "\n",
    "    bias, rmse, coverage, interval = evalStats(estOTs, trueATEOTs, evalSetOTs)\n",
    "    \n",
    "    biasOTs.append(bias)\n",
    "    rmseOTs.append(rmse)\n",
    "    coverageOTs.append(coverage)\n",
    "    intervalOTs.append(interval)"
   ]
  },
  {
   "source": [
    "## (5) Treatment Effect Heterogeneity Level"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heterogeneityLevels = [0, 2, 4, 8]\n",
    "\n",
    "biasHLs = []\n",
    "rmseHLs = []\n",
    "coverageHLs = []\n",
    "intervalHLs = []\n",
    "\n",
    "for heterogeneity in heterogeneityLevels:\n",
    "    estHLs = []\n",
    "    trueATEHLs = []\n",
    "    evalSetHLs = []\n",
    "    for i in range(K):\n",
    "        if heterogeneity == 0:\n",
    "            y, X, trueBetas, featureNames, treatments, cy, ate = dgp(welfare, effect_type='homogeneous')\n",
    "        else:\n",
    "            y, X, trueBetas, featureNames, treatments, cy, ate = dgp(welfare, heterogeneous_select=heterogeneity)\n",
    "\n",
    "        est, X_test = estimate_cf(y, X, treatments)\n",
    "\n",
    "        estHLs.append(est)\n",
    "        trueATEHLs.append(est.ate(X_test)[0])\n",
    "        evalSetHLs.append(X_test)\n",
    "\n",
    "    bias, rmse, coverage, interval = evalStats(estHLs, trueATEHLs, evalSetHLs)\n",
    "    \n",
    "    biasHLs.append(bias)\n",
    "    rmseHLs.append(rmse)\n",
    "    coverageHLs.append(coverage)\n",
    "    intervalHLs.append(interval)"
   ]
  },
  {
   "source": [
    "## (6) Treatment Type (Continuous vs. Discrete)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treatmentTypes = ['binary', 'continuous']\n",
    "\n",
    "biasTTs = []\n",
    "rmseTTs = []\n",
    "coverageTTs = []\n",
    "intervalTTs = []\n",
    "\n",
    "for treatment_type in treatmentTypes:\n",
    "    estTTs = []\n",
    "    trueATETTs = []\n",
    "    evalSetTTs = []\n",
    "    for i in range(K):\n",
    "        y, X, trueBetas, featureNames, treatments, cy, ate = dgp(welfare, treatment_type=treatment_type)\n",
    "        est, X_test = estimate_cf(y, X, treatments)\n",
    "\n",
    "        estTTs.append(est)\n",
    "        trueATETTs.append(est.ate(X_test)[0])\n",
    "        evalSetTTs.append(X_test)\n",
    "\n",
    "    bias, rmse, coverage, interval = evalStats(estTTs, trueATETTs, evalSetTTs)\n",
    "    \n",
    "    biasTTs.append(bias)\n",
    "    rmseTTs.append(rmse)\n",
    "    coverageTTs.append(coverage)\n",
    "    intervalTTs.append(interval)"
   ]
  },
  {
   "source": [
    "## (7) Alignment"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is to implement / have running state of variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "effect_type=\"heterogeneous\"\n",
    "effect_homogeneous=10\n",
    "effect_heterogeneous=2\n",
    "# treatment_type=\"continuous\"\n",
    "treatment_type=\"binary\"\n",
    "treatment_probability=0.5\n",
    "heterogeneous_select=4\n",
    "order=3\n",
    "linearity=\"med\"\n",
    "N=1000\n",
    "overlap=True\n",
    "overlap_percent=0.5\n",
    "\n",
    "featureNames = list(welfare.columns)\n",
    "\n",
    "importantFeatureNames = ['wrkstat', 'race', 'year', 'hrs1', 'income', 'occ80', 'id', 'educ'] # top 8 important features based on Shapley visualization from \n",
    "importantFeatureIndices = []\n",
    "for name in importantFeatureNames:\n",
    "    importantFeatureIndices.append(featureNames.index(name)) \n",
    "\n",
    "# error terms\n",
    "error = np.random.normal(size=(N,1))\n",
    "\n",
    "# data generation\n",
    "cov = welfare.cov()\n",
    "means = welfare.mean(axis=0)\n",
    "X = np.random.multivariate_normal(means.values, cov, size=N, check_valid='warn', tol=1e-8)\n",
    "\n",
    "# linearity specification\n",
    "if linearity != 'full':\n",
    "    select = 0 # select n most important features for interactions and polynomials\n",
    "    poly = PolynomialFeatures(degree=order, interaction_only=False, include_bias=False)\n",
    "\n",
    "    if linearity == \"high\": \n",
    "        select = 2\n",
    "    elif linearity == \"med\": \n",
    "        select = 4\n",
    "    elif linearity == \"low\": \n",
    "        select = 8\n",
    "    else: # if some typo, assume baseline of high\n",
    "        select = 2\n",
    "\n",
    "    poly.fit(X[:, importantFeatureIndices[:select]])\n",
    "    fullData = poly.transform(X[:, importantFeatureIndices[:select]])\n",
    "    fullNames = poly.get_feature_names(input_features=importantFeatureNames[:select])\n",
    "    higherData = fullData[:, -(fullData.shape[1] - select):] # select only higher order\n",
    "    higherNames = fullNames[-(len(fullNames) - select):]\n",
    "\n",
    "    X = np.append(X, higherData, axis=1) \n",
    "    featureNames.extend(list(higherNames))\n",
    "\n",
    "# treatment type\n",
    "if treatment_type == \"binary\":\n",
    "    # randomly assigned treatments with propensity treatment_probability\n",
    "    treatments = np.random.choice([0, 1], size=N, p=[1 - treatment_probability, treatment_probability]).reshape((-1, 1))\n",
    "    if not overlap:\n",
    "        forced = random.sample(list(np.arange(treatments.shape[0])), int(treatments.shape[0] * overlap_percent))\n",
    "        treatments[forced] = 0\n",
    "    treated = treatments > 0\n",
    "    \n",
    "    # generate counterfactual treatment indicator vector\n",
    "    c_treatments = deepcopy(treatments)\n",
    "    c_treatments[treatments == 0] = 1\n",
    "    c_treatments[treatments == 1] = 0\n",
    "\n",
    "else:\n",
    "    treatments = np.random.uniform(size=(N, 1))\n",
    "    if not overlap:\n",
    "        forced = random.sample(list(np.arange(treatments.shape[0])), int(treatments.shape[0] * overlap_percent))\n",
    "        treatments[forced] = 0\n",
    "    treated = treatments > 0.5\n",
    "    c_treatments = deepcopy(treatments)\n",
    "    # to-do\n",
    "    c_treatments[treatments > 0] = 0\n",
    "    c_treatments[treatments == 0] = treatments.mean()\n",
    "    \n",
    "# treatment effect\n",
    "if effect_type == \"homogeneous\":\n",
    "\n",
    "    T = treatments*effect_homogeneous\n",
    "    CT = c_treatments*effect_homogeneous\n",
    "    \n",
    "else: # heterogeneous\n",
    "    # heterogeneous treatment is effect 1 + effect * (sum of first heterogeneous_select important variables)\n",
    "    heterogeneousIndices = importantFeatureIndices[:heterogeneous_select]\n",
    "    \n",
    "    T = 1 + (2*(X[:, heterogeneousIndices].sum(axis=1)))*treatments.ravel()\n",
    "    T = T.reshape(-1, 1)\n",
    "    T[~treated] = np.zeros((~treated).sum())\n",
    "    \n",
    "    CT = 1 + (2*(X[:, heterogeneousIndices].sum(axis=1)))*c_treatments.ravel()\n",
    "    CT = CT.reshape(-1, 1)\n",
    "    CT[treated] = np.zeros((treated).sum()) # everyone that was treated goes to untreated in counterfactual\n",
    "    \n",
    "\n",
    "betas = np.random.normal(size=X.shape[1]).reshape(-1,1)\n",
    "y = T + X@betas + error\n",
    "cy = CT + X@betas + error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.720148792484203"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:, heterogeneousIndices].mean(axis=0).sum() # average across the used columns to check ATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.43845628547201"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = y[treated] - cy[treated] # the treated\n",
    "untmp = cy[~treated] - y[~treated] # the untreated\n",
    "ate = (tmp.mean() + untmp.mean())/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, X, trueBetas, featureNames, treatments, cy, ate = dgp(welfare, N=36501, order=3, treatment_probability=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "est, X_test = estimate_cf(y, X, treatments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python377jvsc74a57bd0254ebfd476bc3fc79ad5c8b426190fe7c8c50894e6b0352342e1dad3387876d5",
   "display_name": "Python 3.7.7 64-bit ('econ21340-env': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}