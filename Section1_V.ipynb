{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dowhy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4146a453b3e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmpl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdowhy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdowhy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCausalModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdowhy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dowhy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import dowhy\n",
    "from dowhy import CausalModel\n",
    "import dowhy.datasets\n",
    "\n",
    "import econml\n",
    "import warnings\n",
    "from copy import deepcopy\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, normalize\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from econml.dml import CausalForestDML\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def fullDisplay():\n",
    "    pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "\n",
    "def defaultDisplay():\n",
    "    pd.reset_option('^display.', silent=True)\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces = dt.fetch_olivetti_faces()\n",
    "X= faces.data\n",
    "\n",
    "# Fit a kernel density model using GridSearchCV to determine the best parameter for bandwidth\n",
    "bandwidth_params = {'bandwidth': np.arange(0.01,1,0.05)}\n",
    "grid_search = GridSearchCV(KernelDensity(), bandwidth_params)\n",
    "grid_search.fit(X)\n",
    "kde = grid_search.best_estimator_\n",
    "\n",
    "# Generate/sample 8 new faces from this dataset\n",
    "new_faces = kde.sample(8, random_state=rand_state)\n",
    "\n",
    "# Show a sample of 8 original face images and 8 generated faces derived from the faces dataset\n",
    "fig,ax = plt.subplots(nrows=2, ncols=8,figsize=(18,6),subplot_kw=dict(xticks=[], yticks=[]))\n",
    "for i in np.arange(8):\n",
    "    ax[0,i].imshow(X[10*i,:].reshape(64,64),cmap=plt.cm.gray)   \n",
    "    ax[1,i].imshow(new_faces[i,:].reshape(64,64),cmap=plt.cm.gray)    \n",
    "ax[0,3].set_title('Original Data',fontsize=20)\n",
    "ax[1,3].set_title('Synthetic Data',fontsize=20)\n",
    "fig.subplots_adjust(wspace=.1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BETA = 10 \n",
    "\n",
    "df = data['df']\n",
    "X = df.drop(['y'], axis=1)\n",
    "Y = df['y']\n",
    "T = df['v0']\n",
    "X_train, X_test, Y_train, Y_test, T_train, T_test = train_test_split(X, Y, T, test_size=0.2)\n",
    "        \n",
    "# specify hyperparams of model\n",
    "est = CausalForest(criterion='mse', n_estimators=1000,       \n",
    "                        min_samples_leaf=1, \n",
    "                        max_depth=100, max_samples=0.5,\n",
    "                        honest=True, inference=True)\n",
    "\n",
    "# fit model\n",
    "est.fit(X_train, T_train, Y_train)\n",
    "\n",
    "predict, sigma = est.predict_and_var(X_test)\n",
    "\n",
    "est, predict, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_cf(y, X, treatments, test_size=0.2, criterion='mse', cv=5):\n",
    "    # split data into train and test sets \n",
    "    X_train, X_test, Y_train, Y_test, T_train, T_test = train_test_split(X, y, treatments, test_size=test_size)\n",
    "        \n",
    "    # specify hyperparams of model\n",
    "    est = CausalForestDML(criterion='het', \n",
    "                            n_estimators=1000,       \n",
    "                            max_samples=0.5,\n",
    "                            discrete_treatment=True,\n",
    "                            honest=True,\n",
    "                            inference=True,\n",
    "                            cv=cv,\n",
    "                            )\n",
    "    # fit model\n",
    "    est.fit(Y_train, T_train, X=X_train, W=None)\n",
    "        \n",
    "    return est, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est_general, X_test_general = estimate_cf(labels, welfare, treatments)\n",
    "est_general.ate_inference(X_test_general)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
